{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import keras\n",
    "from scipy import stats\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import preprocessing\n",
    "from sklearn.metrics import classification_report\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Flatten, Reshape\n",
    "from keras.utils import np_utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# defining the labels in the dataset\n",
    "labels = ['Walking',\n",
    "         'Jogging',\n",
    "         'Sitting',\n",
    "         'Standing',\n",
    "         'Upstairs',\n",
    "         'Downstairs']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To read the file and making a dataset \n",
    "def read_file(file_path):\n",
    "    \n",
    "    # column names for the datasaet\n",
    "    column_names = ['user', 'activity', 'timestamp', 'x-accel', 'y-accel', 'z-accel']\n",
    "    dataset = pd.read_csv(file_path, sep=',', header=None, names = column_names)\n",
    "    dataset['z-accel'].replace(to_replace = r';', value = r'', regex = True)\n",
    "    dataset['z-accel'] = dataset['z-accel'].astype(float)\n",
    "    dataset.dropna(axis=0, how='any', inplace=True)\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To split test and train data\n",
    "def train_test(dataset):\n",
    "    #train,test = train_test_split(dataset, test_size = 0.1)\n",
    "    test = dataset[dataset['user'] > 30]\n",
    "    train = dataset[dataset['user'] <= 30]\n",
    "\n",
    "    return train,test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TO normalize the data set before performing the operation\n",
    "def normalize(dataset):\n",
    "    \n",
    "    dataset['x-accel'] = dataset['x-accel']/dataset['x-accel'].max()\n",
    "    dataset['y-accel'] = dataset['y-accel']/dataset['y-accel'].max()\n",
    "    dataset['z-accel'] = dataset['z-accel']/dataset['z-accel'].max()\n",
    "    \n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_segments_and_labels(dataframe, time_steps, step, label_name):\n",
    "\n",
    "    # x, y, z acceleration as features\n",
    "    features = 3\n",
    "    # Number of steps to advance in each iteration (for me, it should always\n",
    "    # be equal to the time_steps in order to have no overlap between segments)\n",
    "    segments = []\n",
    "    labels = []\n",
    "    for i in range(0, len(dataframe) - time_steps, step):\n",
    "        xs = dataframe['x-accel'].values[i: i + time_steps]\n",
    "        ys = dataframe['y-accel'].values[i: i + time_steps]\n",
    "        zs = dataframe['z-accel'].values[i: i + time_steps]\n",
    "        # Retrieve the most often used label in this segment\n",
    "        label = stats.mode(dataframe[label_name][i: i + time_steps])[0][0]\n",
    "        segments.append([xs, ys, zs])\n",
    "        labels.append(label)\n",
    "\n",
    "    # Bring the segments into a better shape\n",
    "    reshaped_segments = np.asarray(segments, dtype= np.float32).reshape(-1, time_steps, features)\n",
    "    labels = np.asarray(labels)\n",
    "\n",
    "    return reshaped_segments, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = read_file('WISDM_ar_v1.1_raw.txt')\n",
    "column = 'Activity'\n",
    "# Transform the labels from String to Integer via LabelEncoder\n",
    "le = preprocessing.LabelEncoder()\n",
    "# Add a new column with encoded values\n",
    "dataset[column] = le.fit_transform(dataset['activity'].values.ravel())\n",
    "train,test = train_test(dataset)\n",
    "\n",
    "print(\"Total size of Available data set is  \" + str(len(dataset)))\n",
    "print(\"Total size of Train data set is  \" + str(len(train)))\n",
    "print(\"Total size of Test data set is  \" + str(len(test)))\n",
    "\n",
    "# create train dataset\n",
    "x_train, y_train = create_segments_and_labels(train,\n",
    "                                              time_steps = 80,\n",
    "                                              step = 40,\n",
    "                                              label_name = 'Activity')\n",
    "\n",
    "# create test dataset\n",
    "x_test, y_test = create_segments_and_labels(test,\n",
    "                                              time_steps = 80,\n",
    "                                              step = 40,\n",
    "                                              label_name = 'Activity')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_time_periods, total_axis = x_train.shape[1], x_train.shape[2]\n",
    "num_classes = le.classes_.size\n",
    "input_shape = (num_time_periods*total_axis)\n",
    "x_train = x_train.reshape(x_train.shape[0], input_shape)\n",
    "x_test = x_test.reshape(x_test.shape[0], input_shape)\n",
    "\n",
    "print('x_train shape:', x_train.shape)\n",
    "print('input_shape:', input_shape)\n",
    "y_train_hot = np_utils.to_categorical(y_train, num_classes)\n",
    "print('New y_train shape: ', y_train_hot.shape)\n",
    "y_test_hot = np_utils.to_categorical(y_test, num_classes)\n",
    "print('New y_test shape: ', y_test_hot.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model():\n",
    "    \n",
    "    model = Sequential()\n",
    "    model.add(Reshape((80, 3), input_shape=(input_shape,)))\n",
    "    model.add(Dense(100, activation='relu'))\n",
    "    model.add(Dense(100, activation='relu'))\n",
    "    model.add(Dense(100, activation='relu'))\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(num_classes, activation='softmax'))\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = model()\n",
    "print(model.summary())\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "                optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "BATCH_SIZE = 300\n",
    "EPOCHS = 25\n",
    "\n",
    "history = model.fit(x_train,\n",
    "                      y_train_hot,\n",
    "                      batch_size=BATCH_SIZE,\n",
    "                      epochs=EPOCHS,\n",
    "                      validation_split=0.2,\n",
    "                      verbose=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_predict = model.predict(x_test)\n",
    "y_actual = np.argmax(y_predict, axis=1)\n",
    "y_expected = np.argmax(y_test_hot, axis=1)\n",
    "\n",
    "print(y_expected.shape)\n",
    "print(y_actual.shape)\n",
    "print(classification_report(y_expected, y_actual))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
